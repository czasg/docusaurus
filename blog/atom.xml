<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://czasg.github.io/docusaurus/blog</id>
    <title>Czasg Blog</title>
    <updated>2022-03-18T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://czasg.github.io/docusaurus/blog"/>
    <subtitle>Czasg Blog</subtitle>
    <icon>https://czasg.github.io/docusaurus/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[数据库单表十亿量级的优化之路]]></title>
        <id>数据库单表十亿量级的优化之路</id>
        <link href="https://czasg.github.io/docusaurus/blog/2022/3/18/数据库单表十亿量级的优化之路"/>
        <updated>2022-03-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[在常规数据库中，单表数据量到达百万时，查询方面就可能出现性能问题。而当单表数据量达到千万量级时，不仅查询，连索引维护也可能出现问题。]]></summary>
        <content type="html"><![CDATA[<p>在常规数据库中，单表数据量到达百万时，查询方面就可能出现性能问题。而当单表数据量达到千万量级时，不仅查询，连索引维护也可能出现问题。<br/>
<!-- -->此次我们的主角是&quot;单表量级十亿&quot;，本文章主要记录在优化期间的一些思路与实现。</p><h2>背景介绍</h2><ul><li><strong>数据库</strong>：postgres  </li><li><strong>数据库配置</strong>：4c8g + ssd</li><li><strong>表名</strong>：czasg（已脱敏）</li><li><strong>表量级</strong>：10亿  </li><li><strong>表字段</strong>：（已脱敏）<ul><li><strong>key</strong>：主键，类型 text</li><li><strong>value</strong>： 类型 text</li></ul></li><li><strong>业务需求</strong>：每天会有一批数据源，400w 左右，需求就是写入这批数据，并输出实际新增和更新的数据。</li></ul><p>当前线上处理方式比较常规，主要包括下面两个步骤：</p><h4>1、创建临时表，写入数据</h4><p>创建临时表，存储业务待处理数据，总量在 400w 上下。（数据插入过程略）</p><pre><code class="language-sql">CREATE TEMPORARY TABLE tmp_czasg (
    key    text,
    value  text
) ON COMMIT DROP;
</code></pre><p>创建临时表，用于存储最终更新数据。</p><pre><code class="language-sql">CREATE TEMPORARY TABLE tmp_czasg_update (
    key        text,
    value      text,
    old_value  text
) ON COMMIT DROP;
</code></pre><h4>2、join 计算结果集</h4><p>以 key 为连接条件 join 原表，再过滤出 value 不同或者原表 value 为空的数据.</p><pre><code class="language-sql">INSERT INTO tmp_czasg_update 
SELECT newkv.key, newkv.value, kv.value FROM tmp_czasg AS newkv 
LEFT JOIN czasg AS kv ON newkv.key = kv.key 
WHERE newkv.value != kv.value OR kv.value IS NULL;
</code></pre><h4>3、将结果集更新至表 <code>czasg</code></h4><p><code>tmp_czasg_update</code> 中 old_value 为空的表示新增数据，我们将它插入到原表中。</p><pre><code class="language-sql">INSERT INTO czasg SELECT key, value FROM tmp_czasg_update 
WHERE old_value IS NULL;
</code></pre><p><code>tmp_czasg_update</code> 中 old_value 非空的表示更新数据，我们将它更新到原表中。</p><pre><code class="language-sql">UPDATE czasg AS kv SET kv.value = newkv.value FROM 
(SELECT * FROM tmp_czasg_update WHERE old_value IS NOT NULL) AS newkv 
WHERE kv.key = newkv.key;
</code></pre><p>至此业务结束。根据当前线上情况，执行完一次需要 N（N &gt;= 2）小时。</p><p>优化需求：由于该业务会长时间阻塞，影响其他关联业务，<strong>希望减少此阶段的执行时间。</strong></p><h2>优化之路：痛点分析</h2><p>看完上面的背景介绍后，我们应该已经初步了解业务流程了，整理如下图：<br/>
<img src="./data-update.png"/></p><p>可以看出，主要涉及到的操作就是对数据表更新并生成版本更新包。</p><p>现在我们来拆解其中的流程，大致分析下耗时的原因：</p><p>1、创建临时表<br/>
<!-- -->2、数据写入临时表<br/>
<!-- -->3、<strong>关联计算最终待作业的数据</strong><br/>
<!-- -->4、<strong>插入新数据</strong><br/>
<!-- -->5、更新旧数据<br/>
<!-- -->6、导出结果集   </p><p>在上述流程中，主要耗时点就是步骤 3 和 4。<br/>
<!-- -->步骤 3 耗时是因为数据量过大，关联过程无法全程在内存中操作，需要借用磁盘来完成中间计算。<br/>
<!-- -->步骤 4 耗时是因为存在索引，对这个量级的索引进行增删操作都会非常耗时。     </p><p>首先，针对量级过大的问题，解决方案比较统一，那就是进行水平拆分，尽可能减小表的大小。<br/>
<!-- -->其次，针对索引导致的插入/删除耗时，解决方案也比较简单，就是插入/删除前，先 <code>DROP INDEX</code>，执行完操作后，再 <code>CREATE INDEX</code>。<br/>
<!-- -->在这里我们之所以不这样做，是因为重建一张 10亿 量级表的索引，不见得比当前方案快多少。     </p><p>有了大致思路，我们再来想想具体的落地方案。</p><h2>优化之路：表分区</h2><p>针对第一个问题，我们计划选择水平拆分这张大表。<br/>
<!-- -->拆分时要考虑到单表的大小，最好能控制在百万级。<br/>
<!-- -->所以我们可以选择将这张 10亿 大表拆分成 256 张小表，这样平均到每张表的大小就是 400w 左右了。</p><p>一般拆分的方式就是创建多张物理表，然后将数据按照某种规则均匀的插入到这些表中，从而减小单表的大小。<br/>
<!-- -->但我们业务场景并没有特别复杂，在这里我们选择了<strong>表分区</strong>。</p><p>自然而然，<code>key</code> 字段也就成了我们首选的分区条件。<br/>
<!-- -->由于该字段是随机字串，没有规律，只能采用哈希的方式散射到各个分区。<br/>
<!-- -->然后我在分区内创建 <code>key_btree_index_key</code> 索引，此时每个分区的索引大小就可以控制在较小的范围内了。  </p><p>到这里，初步优化方案已经形成。但考虑到随机字串生成的索引，无论在空间还是性能上都有所欠缺，
所以本着一步优化到位的想法，我们决定重新设计字段索引：<br/>
<!-- -->1、新增一个字段 <code>key_crc32</code>，用于存储 <code>key</code> 值计算出的哈希数，此数位于 <code>1 ~ 2^32</code> 之间。<br/>
<!-- -->2、将分区条件设定为 <code>key_crc32</code><br/>
<!-- -->3、创建分区索引 <code>key_crc32_btree_index_key</code>     </p><p>至此，我们的表分区方案初步落地。索引这一块属于过度设计，因为引入了新字段，所以是否属于正向优化，有待考量。</p><p>:::tip 表继承
在 postgres 中，继承可以快速的<strong>关联、区分</strong>多张表，让数据维护更加高效。  </p><p>使用继承时，至少需要有一张源表，然后通过指定源表来创建继承表。<br/>
<!-- -->对于继承表来说，此时的继承表将<strong>拥有源表的全部列</strong>，并且可以自定义额外的列属性。<br/>
<!-- -->对于源表来说，此时的源表将可以检索<strong>自身以及继承表的全部数据</strong>。   </p><p>在这里我们举一个例子，假设你有一张表 <code>cities</code>，里面存放了国内外所有的城市。</p><pre><code class="language-sql">CREATE TABLE cities (
    name      text,   -- 城市名
    altitude  int     -- 城市海拔高度
);
</code></pre><p>可是每个国家只有一个首都城市，现在有业务需求要快速筛分首都城市。<br/>
<!-- -->为了实现这个需求，我们可能需要给这张表加一个字段，或者重新创建一张首都城市的表，这些都是不错的实现方案。<br/>
<!-- -->现在，让我们用继承来实现这个需求：</p><pre><code class="language-sql">CREATE TABLE capitals (
    country  text     -- 所属国
) INHERITS (cities);
</code></pre><p>在这里我们创建继承表 <code>capitals</code>，继承自 <code>cities</code>。这时我们可以这样存储数据：<br/>
<!-- -->1、将首都城市存放至 <code>capitals</code>，并指定所属国。<br/>
<!-- -->2、将其余城市存放至 <code>cities</code>。   </p><pre><code class="language-sql" metastring="title=&quot;查询所有城市中，海拔高于500的城市&quot;" title="&quot;查询所有城市中，海拔高于500的城市&quot;">SELECT * FROM cities WHERE altitude &gt; 500;
</code></pre><pre><code class="language-sql" metastring="title=&quot;查询非首都城市中，海拔高于500的城市&quot;" title="&quot;查询非首都城市中，海拔高于500的城市&quot;">SELECT * FROM ONLY cities WHERE altitude &gt; 500; -- 通过 ONLY 限制仅查 cities 表
</code></pre><pre><code class="language-sql" metastring="title=&quot;查询所有首都城市中，海拔高于500的城市&quot;" title="&quot;查询所有首都城市中，海拔高于500的城市&quot;">SELECT * FROM capitals WHERE altitude &gt; 500;
</code></pre><p>:::</p><p>:::tip 表分区
在 postgres 中，表分区是将一张大表，划分成一些小的物理上的片。这样划分的好处有很多：  </p><ul><li>某些场景下读写性能提升。<strong>减小单表数据量、索引大小</strong>。      </li><li>某些场景下，<strong>批量操作</strong>性能将得到极大提升。     </li><li>可以按实际需求，将价值低的数据块迁移至便宜且较慢的存储介质上。   </li></ul><p>创建表分区的方式有两种：<br/>
<!-- -->第一种，初始化时通过 <code>PARTITION BY</code> 声明该表为分区表，且需要指定分区键，然后就可以创建分区了。</p><pre><code class="language-sql" metastring="title=&quot;声明分区表&quot;" title="&quot;声明分区表&quot;">CREATE TABLE measurement (
    city_id    int not null,
    logdate    date not null,
    peaktemp   int,
    unitsales  int
) PARTITION BY RANGE (logdate); -- 分区支持范围划分、列表划分、哈希划分
</code></pre><pre><code class="language-sql" metastring="title=&quot;创建分区&quot;" title="&quot;创建分区&quot;">CREATE TABLE measurement_y2006m02 PARTITION OF measurement
    FOR VALUES FROM (&#x27;2006-02-01&#x27;) TO (&#x27;2006-03-01&#x27;);
CREATE TABLE measurement_y2006m03 PARTITION OF measurement
    FOR VALUES FROM (&#x27;2006-03-01&#x27;) TO (&#x27;2006-04-01&#x27;);
</code></pre><p>创建分区之后，你对 <code>measurement</code> 的操作都会被重定向到某个分区，比如：<br/>
<!-- -->1、插入一条数据到 <code>measurement</code> 中，当这条数据被<strong>映射到 <code>measurement_y2006m02</code> 时</strong>，
这条数据将被<strong>重定向</strong>到 <code>measurement_y2006m02</code> 分区。<br/>
<!-- -->2、读取一条数据，当这条数据被映射到 <code>measurement_y2006m02</code> 时，查询也会重定向。</p><p>第二种，通过继承实现表分区。</p><pre><code class="language-sql" metastring="title=&quot;创建继承表&quot;" title="&quot;创建继承表&quot;">CREATE TABLE measurement_y2006m02 () INHERITS (measurement);
</code></pre><p>为了实现类似分区表的能力，我们引入触发器：</p><pre><code class="language-sql" metastring="title=&quot;创建触发器与函数&quot;" title="&quot;创建触发器与函数&quot;">CREATE OR REPLACE FUNCTION measurement_insert_trigger()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO measurement_y2008m01 VALUES (NEW.*);
    RETURN NULL;
END;
$$
LANGUAGE plpgsql;

CREATE TRIGGER insert_measurement_trigger
    BEFORE INSERT ON measurement
    FOR EACH ROW EXECUTE FUNCTION measurement_insert_trigger();
</code></pre><p>我们可以看到通过继承与触发器的方式，我们可以间接实现分区表的能力，他相比于声明式分区表，优势就是限制会宽松一些，
比如我们可以为每一个分区创建不同的字段的不同索引，但代价就是更复杂维护逻辑。
:::</p><h2>优化之路：继承式分区表填坑</h2><p>按照预期方案实现后，我拷贝了一份正式环境数据执行测试，结果令人震惊，耗时 3小时 😱😱😱<br/>
<!-- -->就离了个大谱，妥妥的反向优化。 😭😭😭  </p><p>理论和实际差距过大，不得不让我开始反思到底是哪里出现了异常。<br/>
<!-- -->通过日志发现在关联计算、插入新数据、更新数据这些流程都非常耗时。<br/>
<!-- -->但是我们已经设计了分区，减小了每张表的数量和索引大小。所以理论上操作不可能会更耗时。  </p><p>通过进一步打断点输出日志，我发现了继承式分区表的触发器，在执行操作时性能非常拉跨。</p><pre><code class="language-sql" metastring="title=&quot;初始化表分区&quot;" title="&quot;初始化表分区&quot;">CREATE TABLE IF NOT EXISTS origin_0 (CHECK (origin_id = 0)) INHERITS (cloud_search_kvs);
CREATE TABLE IF NOT EXISTS origin_0_0 (CHECK (0 &lt; k_crc32 AND k_crc32 &lt;= 858993459)) INHERITS (origin_0);
CREATE TABLE IF NOT EXISTS origin_0_1 (CHECK (858993459 &lt; k_crc32 AND k_crc32 &lt;= 1717986918)) INHERITS (origin_0);
CREATE TABLE IF NOT EXISTS origin_0_2 (CHECK (1717986918 &lt; k_crc32 AND k_crc32 &lt;= 2576980377)) INHERITS (origin_0);
CREATE TABLE IF NOT EXISTS origin_0_3 (CHECK (2576980377 &lt; k_crc32 AND k_crc32 &lt;= 3435973836)) INHERITS (origin_0);
CREATE TABLE IF NOT EXISTS origin_0_4 (CHECK (3435973836 &lt; k_crc32 AND k_crc32 &lt;= 4294967295)) INHERITS (origin_0);
</code></pre><pre><code class="language-sql" metastring="title=&quot;初始化触发器&quot;" title="&quot;初始化触发器&quot;">CREATE OR REPLACE FUNCTION origin_0_insert_trigger() RETURNS TRIGGER AS $$
BEGIN
 IF (0 &lt; NEW.k_crc32 AND NEW.k_crc32 &lt;= 858993459) THEN INSERT INTO origin_0_0 VALUES (NEW.*);
 ELSIF (858993459 &lt; NEW.k_crc32 AND NEW.k_crc32 &lt;= 1717986918) THEN INSERT INTO origin_0_1 VALUES (NEW.*);
 ELSIF (1717986918 &lt; NEW.k_crc32 AND NEW.k_crc32 &lt;= 2576980377) THEN INSERT INTO origin_0_2 VALUES (NEW.*);
 ELSIF (2576980377 &lt; NEW.k_crc32 AND NEW.k_crc32 &lt;= 3435973836) THEN INSERT INTO origin_0_3 VALUES (NEW.*);
 ELSIF (3435973836 &lt; NEW.k_crc32 AND NEW.k_crc32 &lt;= 4294967295) THEN INSERT INTO origin_0_4 VALUES (NEW.*);
 END IF;
 RETURN NULL;
END;
$$ LANGUAGE plpgsql;
</code></pre><p>触发器的原理就是通过查询条件，将数据重定向到指定分区。<br/>
<!-- -->上面的初始化语句都是简化后的，实际上有 256 个判断条件，而且触发器每次似乎只能执行单条语句，这也导致了整体性能急剧下降。<br/>
<!-- -->我们方案设计上没有太大的问题，但是执行方式有问题，触发器的重定向不适合这种场景，所以我只能尝试自己维护。</p><p>重新优化思路，获取每个分区的数据，直接和目标分区作关联运算，绕开触发器。  </p><pre><code class="language-sql">CREATE TEMPORARY TABLE temp_diff_%[2]d (
    k text,
    v text,
    k_crc32 bigint,
    old_v text
) ON COMMIT DROP;
WITH temp_diff_w AS (SELECT * FROM temp_add WHERE (%[3]d &lt; k_crc32 AND k_crc32 &lt;= %[4]d))
INSERT INTO temp_diff_%[2]d SELECT tadd.k, tadd.v, tadd.k_crc32, tadd.origin_id, kv.v FROM temp_diff_w AS tadd LEFT JOIN origin AS kv ON tadd.k = kv.k AND tadd.k_crc32 = kv.k_crc32 WHERE tadd.v != kv.v OR kv.v IS NULL;
INSERT INTO temp_diff (SELECT * FROM temp_diff;
UPDATE origin AS o SET v = row.v FROM (SELECT * FROM temp_diff_%[2]d WHERE old_v IS NOT NULL) as row WHERE o.k_crc32 = row.k_crc32 AND o.k = row.k;
DROP INDEX IF EXISTS origin_btree_index_key;
INSERT INTO origin (k, v, k_crc32, origin_id) (SELECT k, v, k_crc32, origin_id FROM temp_diff_%[2]d WHERE old_v IS NULL);
CREATE INDEX IF NOT EXISTS origin_btree_index_key ON origin USING btree (k_crc32);
DROP TABLE IF EXISTS temp_diff_%[2]d;
</code></pre><p>以上是部分执行语句（已脱敏），可以看出，第一步就取出了分区数据，然后执行关联运算获取最终结果集，再将结果集数据写入源表即可。  </p><p>再次测试，执行一次大概在 半小时 左右，完美 🤠🤠🤠</p><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍
:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[数据库是否适合容器化部署]]></title>
        <id>数据库是否适合容器化部署</id>
        <link href="https://czasg.github.io/docusaurus/blog/2022/2/22/数据库是否适合容器化部署"/>
        <updated>2022-02-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[随着 Kubernetes 的流行，越来越来的应用被打包成镜像，作为容器部署在 k8s 上。]]></summary>
        <content type="html"><![CDATA[<p>随着 Kubernetes 的流行，越来越来的应用被打包成镜像，作为容器部署在 k8s 上。</p><p>在 k8s 中，pod 是最小的部署单位，内部可以包含多个容器镜像，每一个 pod 都具有有限的生命周期。<br/>
<!-- -->对于无状态服务来说，特别适合在 k8s 上运行，因为无状态服务，是具有弹性的，可以更好的应对 pod 扩缩容、重启等特性。<br/>
<!-- -->相反，对于有状态服务（类似数据库等），则存在一定的瓶颈限制。</p><p>先来看下 k8s 如何部署有状态服务，然后讨论下其优缺点。</p><h2>DaemonSet &amp; StatefulSet</h2><p><strong>DaemonSet</strong> 是 k8s 的控制器资源对象之一，其特点是：   </p><blockquote><p>1、确保每个节点仅启动一个 pod 副本，当有新节点加入集群时，自动为其增加一个 pod。</p></blockquote><p>比较典型的是日志收集服务，比如：我们需要确保每个节点上起一个 <code>logstash</code>，以便回收节点上的服务日志。  </p><p>数据库服务可以在类似场景下使用，比如给某个节点打上标签，确保某个数据库在对应节点上以 DaemonSet 类型启动，这样就可以确保该节点仅有一个 pod 副本。
最后通过 <code>nodeport</code> 的方式对外提供服务。
此时我们的数据库就拥有了稳定的节点 ip 和本地存储。  </p><p>但是这种方式，和直接在物理机上部署的方式差别不大，主要是复用了 k8s 的管理能力。</p><p><strong>StatefulSet</strong> 是 k8s 专为有状态服务设计的资源类型，它拥有：  </p><blockquote><p>1、稳定的网络标识：当服务重启，pod 绑定的 ip 不会变化，但是手动删除或者重建副本时，会重新分配 ip。
创建 headless service 时可以通过 <code>${serviceName-number}.${service}.${namespace}</code> 访问指定 pod。<br/>
<!-- -->2、稳定的持久存储：基于 pvc 实现，每个 pod 会绑定专有的 pvc 存储。<br/>
<!-- -->3、有序的扩缩容：会安装顺序从 0 逐一启动或者重建，此时类似滚动更新的扩容策略不在适用。      </p></blockquote><p>当我们的数据库服务部署 k8s 上时，可以选择 StatefulSet 资源类型。<br/>
<!-- -->它的基本特点，足够支持一个有状态服务的正常运转。</p><h2>性能瓶颈</h2><p>数据库主要是和存储打交道，其性能瓶颈通常在磁盘IO。<br/>
<!-- -->那么快速的磁盘IO，将有助于提升数据库的性能和吞吐量。<br/>
<!-- -->比如：分别部署在 HDD 和 SSD 上的两个数据库，在相同其他条件下，SSD 上的数据库肯定拥有更高的性能。 </p><p>当我们的数据库部署在 k8s 上时，可以通过挂载卷 volume（nfs、ceph）持久化数据，但是这样就不可避免的引入了网络IO，在本身就拉跨的瓶颈上又补一刀。<br/>
<!-- -->所以当我们使用 k8s 部署数据库时，可能需要重点考虑下性能问题。</p><p>除了 nfs、ceph 之外，还有 hostpath 这类本地存储，可以忽略网络影响。
但是这需要我们的副本绑定到固定的节点上，此时就走的类似 DaemonSet 的部署路线了。</p><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍
:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[记一次 golang 升级导致的 goland 不可用]]></title>
        <id>记一次 golang 升级导致的 goland 不可用</id>
        <link href="https://czasg.github.io/docusaurus/blog/2022/2/17/golang升级导致goland不可用"/>
        <updated>2022-02-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[go1.14.1 升级至 go1.17.7，导致 goland 打开后显示无法找到 GOROOT。]]></summary>
        <content type="html"><![CDATA[<p>go1.14.1 升级至 go1.17.7，导致 goland 打开后显示无法找到 GOROOT。</p><p>错误显示：</p><pre><code class="language-text">The selected directory is not a valid home for Go SDK
</code></pre><p>通过 go env 查看后发现相关配置无问题，环境变量也无问题。</p><p>最后找的解决方案：<br/>
<strong>1、</strong>go version 查看自己当前版本<br/>
<strong>2、</strong>编辑 {GOROOT}/src/runtime/internal/sys/zversion.go 文件，写入以下变量</p><pre><code class="language-go">const TheVersion = `go1.17.4`
</code></pre><p><strong>3、</strong>重启 goland，然后点击 setroot 即可。  </p><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍
:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jmeter及性能测试]]></title>
        <id>Jmeter及性能测试</id>
        <link href="https://czasg.github.io/docusaurus/blog/2022/2/2/Jmeter及性能测试"/>
        <updated>2022-02-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[记录 jmeter 工具的使用，以及在性能测试过程中学习到的一些知识点。]]></summary>
        <content type="html"><![CDATA[<p>记录 jmeter 工具的使用，以及在性能测试过程中学习到的一些知识点。</p><h2>性能指标</h2><ul><li>TPS（Transactions Per Second）：每秒事务数，吞吐率</li><li>QPS（Queries Per Second）：每秒查询数</li><li>RT（Response Time）：响应时间，服务延迟</li></ul><p>性能问题，归根结底还是资源问题。常见的瓶颈点：<br/>
<!-- -->1、网络IO<br/>
<!-- -->2、存储IO<br/>
<!-- -->3、CPU、内存<br/>
<!-- -->4、应用  </p><h2>jmeter 结果解读</h2><p>并发线程、响应时间、TPS之间的关联：</p><pre><code class="language-text">TPS = (1s/响应时间) * 并发线程
</code></pre><p>:::note
假设有4个线程，每个线程每秒发起4个请求并响应，此时并发是16而非4
:::</p><pre><code class="language-text" metastring="title=&quot;1个线程&quot;" title="&quot;1个线程&quot;">summary +   5922 in 00:00:30 =  197.4/s Avg:     4 Min:     0 Max:    26 Err:     0 (0.00%) Active: 1 Started: 1 Finished: 0
summary =  35463 in 00:03:05 =  192.0/s Avg:     5 Min:     0 Max:   147 Err:     0 (0.00%)
summary +   5922 in 00:00:30 =  197.5/s Avg:     4 Min:     0 Max:    24 Err:     0 (0.00%) Active: 1 Started: 1 Finished: 0
summary =  41385 in 00:03:35 =  192.8/s Avg:     5 Min:     0 Max:   147 Err:     0 (0.00%)
summary +   5808 in 00:00:30 =  193.6/s Avg:     5 Min:     0 Max:    25 Err:     0 (0.00%) Active: 1 Started: 1 Finished: 0
summary =  47193 in 00:04:05 =  192.9/s Avg:     5 Min:     0 Max:   147 Err:     0 (0.00%)
</code></pre><p>(1000ms/5ms)*1=200TPS</p><pre><code class="language-text" metastring="title=&quot;10个线程&quot;" title="&quot;10个线程&quot;">summary +  11742 in 00:00:30 =  391.3/s Avg:    25 Min:     0 Max:   335 Err:     0 (0.00%) Active: 10 Started: 10 Finished: 0
summary =  55761 in 00:02:24 =  386.6/s Avg:    25 Min:     0 Max:   346 Err:     0 (0.00%)
summary +  11924 in 00:00:30 =  397.5/s Avg:    25 Min:     0 Max:    80 Err:     0 (0.00%) Active: 10 Started: 10 Finished: 0
summary =  67685 in 00:02:54 =  388.5/s Avg:    25 Min:     0 Max:   346 Err:     0 (0.00%)
summary +  11884 in 00:00:30 =  396.2/s Avg:    25 Min:     0 Max:   240 Err:     0 (0.00%) Active: 10 Started: 10 Finished: 0
summary =  79569 in 00:03:24 =  389.6/s Avg:    25 Min:     0 Max:   346 Err:     0 (0.00%)
</code></pre><p>(1000ms/25ms)*10=400TPS</p><h2>分布式压力测试</h2><pre><code class="language-shell" metastring="script title=&quot;启动指令&quot;" title="&quot;启动指令&quot;">/jmeter/bin/jmeter -n -t tmpl.jmx -R 1.1.1.1:8000,1.1.1.2:8000
</code></pre><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍
:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[常用 sql 操作]]></title>
        <id>常用 sql 操作</id>
        <link href="https://czasg.github.io/docusaurus/blog/2022/1/20/常用 sql 操作"/>
        <updated>2022-01-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[整理下 sql 常见的操作。]]></summary>
        <content type="html"><![CDATA[<p>整理下 sql 常见的操作。</p><h2>mysql</h2><h2>postgres</h2><h3>建表</h3><pre><code class="language-sql" metastring="title=&quot;创建普通表&quot;" title="&quot;创建普通表&quot;">CREATE TABLE IF NOT EXISTS my_table (
    k  text,
    v  text
)
</code></pre><pre><code class="language-sql" metastring="title=&quot;创建临时表&quot;" title="&quot;创建临时表&quot;">CREATE TEMPORARY TABLE IF NOT EXISTS my_table (
    k  text,
    v  text
) ON COMMIT DROP;
</code></pre><pre><code class="language-sql" metastring="title=&quot;删除表&quot;" title="&quot;删除表&quot;">DROP TABLE IF EXISTS my_table; -- 普通删除
DROP TABLE IF EXISTS my_table CASCADE; -- 级联删除
</code></pre><h3>索引</h3><pre><code class="language-sql">CREATE INDEX IF NOT EXISTS my_table_hash_index USING HASH(k);
DROP INDEX IF EXISTS my_table_hash_index;
</code></pre><h3>插入</h3><pre><code class="language-sql">INSERT INTO my_table (k, v) VALUES (&#x27;k&#x27;, &#x27;v&#x27;); -- 插入单条
INSERT INTO my_table (SELECT * FROM same_table); -- 相同结构表之间的批量插入
INSERT INTO my_table (k, v) (SELECT k, v FROM other_table); -- 不同结构表之间的批量插入
</code></pre><h3>更新</h3><pre><code class="language-sql">UPDATE my_table SET v = &#x27;new_v&#x27; WHERE k = &#x27;new_k&#x27;; -- 更新一次
UPDATE my_table AS o SET v = new_table.v FROM (SELECT * FROM other_table) as new_table WHERE o.k = new_table.k; -- 批量更新
</code></pre>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[redis 整理]]></title>
        <id>redis 整理</id>
        <link href="https://czasg.github.io/docusaurus/blog/2022/1/18/redis 整理"/>
        <updated>2022-01-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[整理下 redis 常见知识点。]]></summary>
        <content type="html"><![CDATA[<p>整理下 redis 常见知识点。</p><p>:::note redis 单线程理解
在 6.0 版本之前，redis 内部的网络IO和键值对的读写是在同一个线程中完成的。<br/>
<!-- -->但类似数据持久化、主从同步等，都是多线程完成的。所以本质上，redis 并不是一个绝对的单线程服务。
而官方之所以这样描述，也只是因为他的核心逻辑都是单线程实现的，然后还能提供这么高的并发，整体听上去就很牛了。</p><p>在 6.0 版本之后，redis 在网络IO部分引入了多线程，而键值对的读写则还是由单线程完成。  </p><p>:::</p><h2>常见数据类型</h2><ul><li>string</li><li>hash</li><li>list</li><li>set</li><li>zset</li><li>bitmap</li><li>hyperloglog</li><li>geo</li></ul><h2></h2>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[nginx 配置说明]]></title>
        <id>nginx 配置说明</id>
        <link href="https://czasg.github.io/docusaurus/blog/2022/1/12/nginx配置说明"/>
        <updated>2022-01-12T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[记录一下 nginx 常见配置说明和部署模板（docker、k8s）。]]></summary>
        <content type="html"><![CDATA[<p>记录一下 nginx 常见配置说明和部署模板（docker、k8s）。</p><p>nginx 是一个高性能的 web 服务器，同时也能提供负载均衡和反向代理服务。</p><h2>nginx 常用指令</h2><ul><li>-c：用于指定一个配置文件</li><li>-t：用于测试配置是否可用</li><li>-s：用于发送信号，包括：stop, quit, reopen, reload。eg：<code>nginx -s reload</code></li></ul><pre><code class="language-shell" metastring="script title=&quot;指定并测试配置文件是否正确&quot;" title="&quot;指定并测试配置文件是否正确&quot;">nginx -t -c ./nginx.conf
</code></pre><h2>nginx 核心模块</h2><p>nginx 的核心模块主要有三个：</p><ul><li>主模块：管理 nginx 基本功能的模块</li><li>事件模块：管理 nginx 处理连接请求的模块</li><li>HTTP 模块：管理 nginx 处理 http 请求的模块</li></ul><h3>主模块</h3><pre><code class="language-text">user  nginx;  # 用户
pid  /var/run/nginx.pid;  # 进程ID
worker_processes  auto;  # 工作进程，可以指定具体数字
error_log  /var/log/nginx/error.log notice;  # 指定日志和错误级别，包含：debug、info、notice、warn、error、crit

worker_cpu_affinity  none;  # 用于绑定 worker 进程与 cpu
</code></pre><h3>事件模块</h3><pre><code class="language-text">events {
    worker_connections  1024;  # 每个工作进程最大链接数。乘以 worker_processes 就是该 nginx 服务的最大连接数
}
</code></pre><h3>HTTP 模块</h3><h4>location 路由匹配规则及顺序</h4><ul><li><code>=</code>: 精确匹配</li><li><code>^~</code>: 优先前缀匹配</li><li><code>~</code>: 正则匹配，区分大小写</li><li><code>~*</code>: 正则匹配，不区分大小写</li><li><code>!~</code>: 正则匹配，区分大小写</li><li><code>!~*</code>: 正则匹配，不区分大小写</li><li><code>/route</code>: 普通前缀匹配</li><li><code>/</code>: 通用匹配</li></ul><blockquote><p>精确匹配 &gt; 优先前缀匹配 &gt; 正则匹配 &gt; 普通前缀匹配 &gt; 通用匹配</p></blockquote><pre><code class="language-text">http {
    server {
        # 优先匹配依次往下
        location = /route { return 200; }
        location ^~ /route { return 201; }
        location ~ ^/route { return 202; }
        location /route { return 203; }
        location / { return 204; }
    }
}
</code></pre><h4>upstream 负载均衡策略</h4><ul><li>轮询：在每个服务之间轮询请求</li><li>weight：按指定权重比例在服务之间请求，默认 weight 为 1</li><li>ip_hash：按照请求 IP 计算 Hash，保证每次请求都访问同一个服务</li><li>fair：按照后端的响应时间来分配（三方插件实现）</li></ul><pre><code class="language-text">http {
    # 负载均衡
    upstream serverName {
        # ip_hash;  # 指定 ip_hash 负载均衡策略
        server 10.251.10.10:8080 weight=2;  # 指定权重比例
        server 10.251.10.10:8081 down;  # down 表示服务下线
        server 10.251.10.10:8082;
        server 10.251.10.10:8083 backup;  # backup 表示备用，当其他机器 down 或者压力比较大时，流量会走到此服务
        # fair;  # 指定 fair 负载均衡策略
    }

    server {
        listen  80;  # 监听端口
        server_name  _;  # 不启用域名检测

        location / {
            proxy_pass  http://serverName;  # 指定 upstream 名字即可
        }
    }
}
</code></pre><h4>http 客户端与服务端参数配置</h4><pre><code class="language-text">http {
    server {
        client_body_timeout  60s;  # 定义读取客户端请求体的超时
        client_body_buffer_size  8k;  # 设置读取客户端请求体的缓冲区大小（超过则存储到临时文件中）
        client_header_timeout  60s;  # 定义读取客户端请求头的超时
        client_header_buffer_size  1k;  # 设置读取客户端请求头的缓冲区大小
        client_max_body_size  0;  # 数据最大传输限制
        proxy_request_buffering  off;  # 默认开启，作用是缓冲请求。关闭后请求会立即转发到后端服务
        proxy_buffering  off;  # 对代理服务器的响应内容缓冲
        proxy_buffer_size  4k;  # 从代理服务器获取部分响应后进行缓冲
        proxy_buffers  8 4k;  # 从被代理的后端服务器取得的响应内容，会缓冲到这里
        proxy_connect_timeout  60s;  # 与后端服务建立连接的超时时间
        proxy_send_timeout  60s;  # 向后端传输请求的超时时间
        proxy_read_timeout  60s;  # 从后端读取响应的超时时间
        proxy_set_header  Host $proxy_host;

        proxy_set_header  Host $proxy_host;

        location / {
            proxy_set_header  Auth &quot;auth-key&quot;;
            proxy_pass  http://serverName;
        }
    }
}
</code></pre><pre><code class="language-text">http {
    server {
        location /index {
            index  index.html;  # 首页，即未指定后续路径时，匹配首页
        }
        location /try_files {
            root  /static
            try_files  $uri index.html;  # 依次尝试。/try_files/file -&gt; /try_files/static/file
        }
        location /alias/ {
            alias  /a/new/route/;  # 请求路径 /alias/files 等效于 /a/new/route/files，会替换掉匹配路由
        }
        location /root {
            root  /a/new/route/;  # 请求路径 /root/files 等效于 /a/new/route/root/files，会保留匹配路由
            # proxy_pass  http://serverName/;  # /root/index.html -&gt; http://serverName/index.html
            # proxy_pass  http://serverName;  # /root/index.html -&gt; http://serverName/root/index.html
        }
        location /rewrite {
            rewrite  ^/rewrite/permanent/(.*) http://serverName/$1 permanent;  # 301 永久重定向
            rewrite  ^/rewrite/redirect/(.*) http://serverName/$1 redirect;  # 302 临时重定向
            rewrite  ^/rewrite/last/(.*) http://serverName/$1 last;  # 实现重定向
            rewrite  ^/rewrite/break/(.*) http://serverName/$1 break;  # 实现重定向
        }
    }
}
</code></pre><h4>mirror</h4><pre><code class="language-text">http {
    server {
        location /mirror {
            mirror  /internal;  # mirror 实现流量拷贝
            proxy_pass  http://serverName;
        }

        location /internal {
            internal;  # 表示仅被内部请求发现
            proxy_pass  http://serverName;  # 指定 upstream 名字即可
        }
    }
}
</code></pre><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍<br/>
<!-- -->:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[初步了解 k8s 资源配额]]></title>
        <id>初步了解 k8s 资源配额</id>
        <link href="https://czasg.github.io/docusaurus/blog/2021/12/23/k8s资源配额"/>
        <updated>2021-12-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[<!--]]></summary>
        <content type="html"><![CDATA[<p>用一台服务器同时跑多个任务时，最常发生的事故就是资源抢占问题。比如端口、cpu、内存、存储等都可能会发生冲突。<br/>
<!-- -->而 k8s 作为容器编排工具，则提供了资源配额来避免这类问题。</p><h2>资源配额 ResourceQuota</h2><p>资源配额用于声明命名空间中所有 pod 的最终资源情况，一旦命名空间中启用了 ResourceQuota，则部署 pod 时必须设定请求值（request）和约束值（limit），否则配额系统将拒绝 pod 的创建。<br/>
<!-- -->资源配额在 k8s 集群中默认是支持的，当一个命名空间中存在一个 ResourceQuota 对象时，该命名空间的资源配额就是开启的。   </p><pre><code class="language-yaml" metastring="title=&quot;资源配额：ResourceQuota.yaml&quot;" title="&quot;资源配额：ResourceQuota.yaml&quot;">apiVersion: v1
kind: ResourceQuota
metadata:
  name: demo
spec:
  hard:
    requests.cpu: &quot;1&quot;
    requests.memory: 1Gi
    limits.cpu: &quot;2&quot;
    limits.memory: 2Gi
</code></pre><p>此配置表示： </p><ul><li>每个容器必须有 memory、cpu 的请求和限制。</li><li>所有容器的 memory 请求总和不能超过 1 GiB。</li><li>所有容器的 memory 限制总和不能超过 2 GiB。</li><li>所有容器的 cpu 请求总和不能超过 1 cpu。</li><li>所有容器的 cpu 限制总和不能超过 2 cpu。</li></ul><pre><code class="language-yaml" metastring="title=&quot;部署示例：pod.yaml&quot;" title="&quot;部署示例：pod.yaml&quot;">apiVersion: v1
kind: Pod
metadata:
  name: demo
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        cpu: &quot;200m&quot;
        memory: &quot;400Mi&quot;
      requests:
        cpu: &quot;100m&quot;
        memory: &quot;200Mi&quot;
</code></pre><h2>requests/limits</h2><p>requests 表示请求值，而 limits 则表示约束值。<br/>
<!-- -->一般情况下 requests 是小于 limits 的，这里涉及到服务质量等级概念：</p><table><thead><tr><th>服务质量等级</th><th>说明</th></tr></thead><tbody><tr><td>Guaranteed（完全可靠的）</td><td>pod 中全部容器配置了资源配额，且 requests == limits，或者只设置了 limits，此时默认 requests 等于 limits。</td></tr><tr><td>Burstable（弹性波动、较可靠的）</td><td>pod 中部分容器配置了 requests 和 limits。</td></tr><tr><td>BestEffort（尽力而为、不太可靠的）</td><td>pod 中容器未配置 requests 和 limits。</td></tr></tbody></table><p>当我们配置了 requests 请求值，那么部署时，目标节点需要拥有多余该请求值的空闲资源，才可以部署此服务。如果没有符合条件的节点，那么该服务将无法部署。<br/>
<!-- -->当我们未配置 limits 约束值时，limits 默认就是当前节点的资源上限。   </p><h2>常使用的配额类型</h2><p>资源配额的种类有很多，最常使用的配额主要有三类：</p><ul><li>cpu：中央处理器</li><li>memory：内存</li><li>ephemeral-storage：存储</li></ul><p>大部分资源类型都可以通过标准的模板进行配额设置，如：  </p><ul><li><code>count/&lt;resource&gt;.&lt;group&gt;</code>：用于非核心 core 组的资源</li><li><code>count/&lt;resource&gt;</code>：用于核心组的资源</li></ul><table><thead><tr><th>配额</th><th>说明</th></tr></thead><tbody><tr><td>count/pods</td><td>在该命名空间中允许存在的非终止状态的 pod 总数上限。</td></tr><tr><td>count/services</td><td>在该命名空间中允许存在的 service 总数上限。</td></tr><tr><td>count/configmaps</td><td>在该命名空间中允许存在的 configmaps 总数上限。</td></tr></tbody></table><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍<br/>
<!-- -->:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[记一次 golang cmd 模块导致的进程泄漏]]></title>
        <id>记一次 golang cmd 模块导致的进程泄漏</id>
        <link href="https://czasg.github.io/docusaurus/blog/2021/12/22/golang-cmd模块导致的进程泄漏"/>
        <updated>2021-12-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[一个稳定跑了一年多的项目，近期突然被运维告知存在进程泄漏，带着懵逼的心情开始了排查...]]></summary>
        <content type="html"><![CDATA[<p>一个稳定跑了一年多的项目，近期突然被运维告知存在进程泄漏，带着懵逼的心情开始了排查...</p><h2>僵尸进程拖垮服务器</h2><p>首先来看下运维提供的截图（屏蔽了部分敏感信息）：<br/>
<strong>1、大面积僵尸进程（进程状态 Z）</strong><br/>
<img src="./%E8%BF%9B%E7%A8%8B%E6%B3%84%E6%BC%8F1.png"/>  </p><p><strong>2、统计进程数近30w</strong><br/>
<img src="./%E8%BF%9B%E7%A8%8B%E6%B3%84%E6%BC%8F2.png"/>  </p><p>并且僵尸进程持续增长，最终拖垮服务器。</p><p>:::note 僵尸进程
一个进程的退出，需要被它的父进程或者相关联的进程接收，否则会一直阻塞，成为僵尸进程。</p><p>常见的僵尸进程产生场景有：<br/>
<!-- -->1、父进程还在执行，子进程退出了，此时子进程的退出没有被父进程接收到，导致当前进程成为僵尸进程。<br/>
<!-- -->2、子进程还在执行，父进程退出了，此时子进程的退出无法被其他进程接收，导致成为孤儿进程，也是僵尸进程的一种。     </p><p>僵尸进程状态为：Z<br/>
<!-- -->以 <code>&lt;defunct&gt;</code> 标记指明<br/>
<!-- -->:::</p><h2>代码分析</h2><pre><code class="language-golang" metastring="title=&quot;调用 cmd 模块&quot;" title="&quot;调用">ctxTimeout, cancel := context.WithDeadline(ctx, cfg.ExpiredAt)
defer cancel()
cmd := exec.CommandContext(ctxTimeout, &quot;/third_party/aria2c&quot;, args...)
</code></pre><p>在上述代码能够看到，调用 <code>cmd</code> 时传入了 <code>ctxTimeout</code>，并且函数栈退出时调用了 <code>cancel</code>。<br/>
<!-- -->看到这里，我们可以大致确认，<code>cmd</code> 最终肯定是可以退出的。但是就是这么一段简单的代码，仍然发生了进程泄漏。</p><p>我们需要注意的是，cmd 是 golang 的基础包，其启动方式有两种，如下：</p><pre><code class="language-golang" metastring="title=&quot;cmd 启动方法&quot;" title="&quot;cmd">{
    cmd.Start()
    cmd.Wait()
}
{
    cmd.Run()  // 等价于 start + wait
}
</code></pre><p>继续往下排查：</p><pre><code class="language-golang">ctxTimeout, cancel := context.WithDeadline(ctx, cfg.ExpiredAt)
defer cancel()
cmd := exec.CommandContext(ctxTimeout, &quot;/third_party/aria2c&quot;, args...)

path, err := consumeOutput(cmd, ctxTimeout, cfg)
if err != nil {
    retrun err
}
</code></pre><p><code>consumeOutput</code> 函数中涉及到一些日志输出的读取与分析，这里就不具体展示了，但这个函数的<strong>最后一行调用了</strong> <code>cmd.Wait()</code>。<br/>
<!-- -->看到这里，问题的起因就大致有点眉目了。</p><p>我们来看下 <code>cmd.Wait()</code> 的文档，最后一行写着：</p><pre><code class="language-text">Wait releases any resources associated with the Cmd.
</code></pre><p>所以可以初步判定，就是因为该函数调用过程中出现异常，没有执行 <code>cmd.Wait()</code> 函数就直接退出，导致 cmd 相关的子进程资源没有及时释放，从而造成线上大量僵尸进程。</p><h2>解决方法</h2><p>老项目，怎么简单怎来。<br/>
<!-- -->这个问题说到底，还是 <code>cmd</code> 模块的使用方法不正确导致的，项目能稳定运行一年多，也只能归结于运气好...</p><p>直接在 <code>consumeOutput</code> 函数调用后主动加上 <code>cmd.Wait()</code> 就行了，虽然不优雅，但能解决问题。</p><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍<br/>
<!-- -->:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[cpu 调度]]></title>
        <id>cpu 调度</id>
        <link href="https://czasg.github.io/docusaurus/blog/2021/12/21/cpu调度"/>
        <updated>2021-12-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[<!--]]></summary>
        <content type="html"><![CDATA[<p>说到 cpu 调度，我们第一时间想到的可能是时间片、分时调度等关键词。
要说更具体的内容，可能就比较模糊了，因为涉及到了操作系统的底层知识，平常工作中接触的比较少。</p><p>这篇文章也仅仅做个简单的了解。</p><h2>基础概念</h2><h4><strong>cpu：上下文切换</strong></h4><p>每一个进程启动，都需要有一个执行指令入口，即使该进程被暂停调度，也会有一个即将被执行的指令入口。
对于这些即将被执行的指令，我们将他们的地址信息保存在 cpu 的寄存器中。<br/>
<!-- -->此时，这些寄存器中的信息，就是我们常说的 cpu 上下文。</p><p>所以简单点理解就是，将当前任务的上下文保存，并将下一个执行任务的上下文加载到寄存器中的过程，称之为 cpu 的上下文切换。
当然，实际过程更为复杂。</p><p>:::note 用户级和系统级的上下文
我们常说的上下文更多的是用户级别的，他和系统级上下文保存内容有所不同：  </p><ul><li>用户级上下文：程序运行时堆栈信息、数据块、代码块等。  </li><li>系统级上下文：进程标识信息、控制信息等。
:::</li></ul><h4><strong>cpu：上下文信保存的方式</strong></h4><p>操作系统给进程分配资源时，会分配一个内核空间，这部分空间只能够由内核代码访问。<br/>
<!-- -->执行上下文切换时，会将上下文信息保存在内核空间中，等待下次调度时，再重新加载到 cpu 中。</p><h4><strong>cpu：上下文切换的方式</strong></h4><p>执行上下文切换，需要由 cpu 来执行。但由于当前 cpu 已经被进程占用了，所以需要一种策略，来获取 cpu 的执行权限。获取的方式主要有两种：</p><p>1、协作式策略<br/>
<!-- -->由进程主动让出 cpu</p><p>2、抢占式策略<br/>
<!-- -->依赖硬件的定时中断机制，操作系统初始化时向硬件注册中断回调，以便当硬件发生中断时，硬件会将 cpu 的处理权交还给操作系统。<br/>
<!-- -->这样，操作系统就可以通过中断回调的方式来获取 cpu 的调度权。</p><h2>最简单的调度：FIFO</h2><p>说到最简单的调度策略，那肯定是先进先出（FIFO）策略了。<br/>
<!-- -->FIFO 的意思就是先来的任务先处理，等当前任务执行完成后，再执行下一个任务。</p><p>他的优点就是简单明了，缺点也很明显，就是正在执行中的任务会阻塞后面的任务。<br/>
<!-- -->特别当有多个任务时：</p><ul><li>执行时间较长的任务可能会阻塞执行时间较短的任务</li><li>优先级低的任务可能会阻塞优先级高的任务</li></ul><h2>最短任务优先：SJF</h2><p>SJF 全称 Short Job First，就是执行时间短的任务优先执行。</p><p>他相较于 FIFO 来说，弥补了一部分调度策略的缺陷。<br/>
<!-- -->当一批任务到来时，可以优先处理执行时间较短的任务。<br/>
<!-- -->但是当执行时间较长的任务正在被处理，此时再投放执行时间较短的任务，这些短任务仍然会被阻塞。</p><h2>最短时间优先完成：STCF</h2><p>STCF 全称 Shortest Time-to-Completion First，就是当运行时间较短的任务达到时，会中断当前任务，转而执行这批短时间任务。<br/>
<!-- -->很显然，这是 SJF 的一种优化。</p><p>在这里我们其实可以看到，不管是 FIFO、SJF 还是 STCF，这些调度策略的本质都是先执行一个任务，再执行下一个任务。    </p><p>当完成一个短任务所需要的时间也很长时，这个时候，对于每一个任务，调度执行的时间将不可避免的被延长。   </p><h2>基于时间片的轮询</h2><p>既然，这种完成一个任务再完成下一个任务的调度策略这么不友好，那么我们就让这些任务一起执行。</p><p>原理也比较简单：<br/>
<!-- -->1、给每个任务分配一个时间片，时间片表示该任务被 cpu 调度执行的时间。<br/>
<!-- -->2、当时间片的调度时间结束后，cpu 会中断该任务，切换到下一个任务。<br/>
<!-- -->3、通过 round robin 轮询的策略，对每一个任务执行调度。    </p><h2>多级反馈队列：MLFQ</h2><p>MLFQ 全称 Multi-Level Feedback Queue</p><p>该策略综合考虑了优先级和轮询的特点，此时调度策略类似：<br/>
<!-- -->1、任务优先级高的任务先执行<br/>
<!-- -->2、优先级相同的任务轮询调度执行  </p><h2>linux完全公平调度：CFS</h2><p>CFS 全称 Completely Fair Scheduler</p><p>CFS 希望将 cpu 公平的平均分到每个任务。为了实现平均能力， CFS 引入了计数的概念。
每个任务都维护有一个 <code>vruntime</code> 的值，用来表示调度执行的时间，每次调度执行后，该值都会加上此次调度时间的值。
每一次调度，操作系统会选择 <code>vruntime</code> 值最小的优先调度。</p><p>在 CFS 种，调度执行时间片不是固定的，linux 提供了可配参数 <code>sched_latency</code>，调度时间片就是：<code>time_slice = sched_latency / n</code>，
这里的 n 就是任务数，即任务数越多，时间片越小。</p><p>除此之外，linux 也提供了权重来标记任务的优先级，优先级高的任务将分配有更多的执行时间片。</p><p>:::note 权重机制
linux 提供了权重机制，用于标记任务的优先级。<br/>
<!-- -->优先级高的任务，将被分配更多的执行时间片。<br/>
<!-- -->在权重机制下，也不再是取 <code>vruntime</code> 值最小的优先调度了，会有新的算法引入权重值。
:::</p><p>:::note CFS 存储结构：红黑树
在文中已经说明，CFS 调度时会选择 <code>vruntime</code> 最小的值，此时为了性能，采用的是红黑树来存储 <code>vruntime</code> 数据。<br/>
<!-- -->红黑树在插入和删除时，操作复杂度为 log(n)<br/>
<!-- -->而有序链表的操作复杂度为 O(n)
:::</p><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍<br/>
<!-- -->:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[字符编码]]></title>
        <id>字符编码</id>
        <link href="https://czasg.github.io/docusaurus/blog/2021/12/15/字符编码"/>
        <updated>2021-12-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[字符串是使用的最多的数据类型之一。]]></summary>
        <content type="html"><![CDATA[<p>字符串是使用的最多的数据类型之一。<br/>
<!-- -->如果说在使用过程中会遇到什么问题，那可以说，大部分都是编码方面的问题了。</p><h2>ascii 编码</h2><blockquote><p>字符编码的本质，是数字到字符串的映射</p></blockquote><p>最早的编码是 ascii 编码，它是一个包含 127 个字符的字符集，可以编码字母和特殊字符。</p><p>:::note 比特与字节
由于计算机底层只能够处理数字，如果要处理类似文本图片等数据，就需要先将这些数据转化为数字才能够继续处理。<br/>
<!-- -->最早计算机在设计时采用 8 个比特（bit）作为一个字节（byte），所以一个字节能表示的最大数字就是 255，二进制表示 <code>1111 1111</code></p><p>如果需要表示更大的数，就需要更多的字节
:::</p><h2>unicode 编码</h2><p>随着计算机的发展，单一的 ascii 编码已经无法满足多元的文化需求。<br/>
<!-- -->计算机需要支持更多的编码集，例如：中国制定有 gb2312，其他国家也有类似的编码集。</p><p>虽然每个国家都实现了特定的编码，但是当一个文本中混合了多个国家的语言时，就不可避免的出现局部乱码。</p><p>为了解决这个问题，业界决定使用统一的 unicode 编码。<br/>
<!-- -->即将所有国家的编码收录到一个字符集中。</p><h2>utf-8 编码</h2><p>虽然 unicode 可以解决乱码问题，但是会出现一个这样的问题：<br/>
<!-- -->中英混合时，一个中文可能需要占据两个字节，这个时候为了正确编码，原本只需要占据一个字节的英文，不得不占据两个字节。<br/>
<!-- -->从而造成了资源浪费。</p><p>为了解决这个缺陷，基于 unicode 编码又推出了可变长编码 utf-8</p><p>即 utf-8 会根据实际类型选择最佳的字节占位，从而节省空间。</p><h2>计算机通用编码方式</h2><p>搞清楚了 ascii、unicode、utf-8 编码，我们梳理下计算机通用的编码方式：<br/>
<!-- -->1、在计算机内存中，统一使用 unicode 编码<br/>
<!-- -->2、当需要保存到硬盘或者传输的时候，转化为 utf-8 编码   </p><p>以我们的 txt 文本为例：<br/>
<!-- -->1、打开文本：数据以 unicode 编码，以便展示内容。<br/>
<!-- -->2、保存文本：数据以 utf-8 编码，以便节约空间。</p><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍<br/>
<!-- -->:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[基于 gRPC 实现负载均衡]]></title>
        <id>基于 gRPC 实现负载均衡</id>
        <link href="https://czasg.github.io/docusaurus/blog/2021/12/13/gRPC负载均衡"/>
        <updated>2021-12-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[本文主要介绍在 k8s 环境下 gRPC 服务在实现负载均衡时遇到的问题和解决思路。]]></summary>
        <content type="html"><![CDATA[<p>本文主要介绍在 k8s 环境下 gRPC 服务在实现负载均衡时遇到的问题和解决思路。</p><h2>概述</h2><p>我们的 gRPC 应用最初使用 <code>DaemonSet</code> 类型的资源对象部署在 k8s 上。通过污点和容忍配置，使得单台 Node 节点不在部署其他 Pod 应用。
而该 gRPC 应用，则通过 <code>NodePort Service</code> 的方式对外暴露服务。</p><p>在这种场景下直接对外暴露固定 IP 提供服务，业务流量全部打到单台宿主机上，所以也不存在负载均衡的说法。</p><p>随着业务压力逐渐增大，单点提供的能力已逐渐达到极限，我们采用水平拓展的方式，部署了多台单节点服务。也算暂时抗住了压力。但随后的暴露的问题，也让我们被迫选择了重构。</p><p>:::note 为什么选择重构
业务持续拓展，引入多节点的问题也逐渐暴露出来，当前服务架构下，多节点之间的数据一致性完全不能保证，人工运维简直不要太恶心，无奈只能重构。这里我们不针对此展开。<br/>
<!-- -->:::</p><p>服务改造升级完毕后，完全兼容历史 gRPC 接口，在 k8s 的配置上也有部分改动，以前的 <code>DaemontSet</code> 资源变更为 <code>Deployment</code> 资源，而 <code>NodePort Service</code> 资源则变更为 <code>ClusterIP Service</code> 资源。
项目正式进入到了提测阶段。</p><p>功能上基本没有太大的问题，毕竟引用了相同的 gRPC 接口文件。</p><p>问题主要出现在了负载均衡上，在多副本的场景下，发生了严重的流量倾斜，具体表现就是某个副本的压力非常高，而其他副本的压力很小，进而导致服务整体异常。</p><p>:::tip 简单分析
gRPC 是基于 HTTP2.0 实现的长连接，且默认没有超时，这种长连接能够大量减少 TCP 连接管理所带来的开销，但也破坏了标准的连接级的负载均衡。因为连接已经建立且不断开，也无法再进一步负载均衡了。  </p><p>那么再回到上述场景，显然就是客户端与某个具体的服务建立了长连接，而连接又不会断开，从而导致了持续的流量倾斜问题。
:::</p><p>选择有效的 gRPC 负载均衡方案，是解决我们当前问题的核心。</p><p>具体解决的方法有多种，我们主要将其分为：  </p><ul><li>用户侧的负载均衡</li><li>服务侧的负载均衡</li></ul><h2>gRPC 负载均衡 - 用户侧</h2><p>用户侧的 gRPC 负载均衡，是通过 DNS，使用户解析出全部的 gRPC 服务地址，然后用户自己实现负载策略。
最简单的策略就是与每个服务建立 gRPC 连接，然后轮询访问，实现 rr 负载。</p><p><img src="client-load-balancer.png"/></p><p>该方案实施起来比较容易，但对于用户侧有一定的要求。如：</p><ul><li>安全方面，要考虑用户的可靠性。</li><li>更新策略，解析 DNS 虽有现成的方案，但是常规方案一般无法探知到后续新创建的服务，所以需要设计更新策略。</li></ul><h2>gRPC 负载均衡 - 服务侧</h2><p>服务侧的 gRPC 负载均衡，需要引入一个负载均衡代理，我们称之为 <code>Load Balancer</code>，
用户向 LB 发起 RPC 请求，然后由该 LB 将 RPC 分配到一个可用的后端服务器上，
由该服务器提供 gRPC 服务，并将负载情况报告给 LB，进一步补全 LB 的负载信息。
<img src="server-load-balancer.png"/></p><p>在该方案中，负载均衡由 LB 统一管理，有一定的实施难度。<br/>
<!-- -->除非能找到高 star 的开源项目，不然开发与运维就是一笔不小的投入。</p><h2>方案选择</h2><p>考虑到实施难度，我们选择了用户侧的负载均衡方式。</p><p>首先要解决 DNS 的解析与负载，这一块，官方已经封装了 gRPC SDK，故用户可以通过升级 SDK 实现该功能。问题就在如何检测更新这一块了。</p><p>解决的方案也有两种：<br/>
<!-- -->1、服务端实现：MaxConnectionAge<br/>
<!-- -->2、客户端实现：KubeResolver   </p><p>:::info MaxConnectionAge
MaxConnectionAge 是 gRPC 服务端的参数，用于指定长连接最大保持时间。<br/>
<!-- -->配置该参数会使得长连接变成“短”连接，故性能会有一定的降低。</p><p>该方案的原理是，通过定期释放连接，使得客户端重新解析 DNS 获取最新的服务地址。
:::</p><p>:::info KubeResolver
该方案的原理是通过监听 k8s 的 api 资源状态，实时获取 gRPC 服务资源信息，从而实现检测更新。
:::</p><p>综合考虑后，最终选择了 MaxConnectionAge 方案，因为此方案仅仅通过适配参数就可以完成预期功能，虽然略显僵硬，但实际上效果还不错。</p><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍<br/>
<!-- -->:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[关于副业的一些思考]]></title>
        <id>关于副业的一些思考</id>
        <link href="https://czasg.github.io/docusaurus/blog/2021/12/10/关于副业的一些思考"/>
        <updated>2021-12-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[所谓副业，本质上就是打多份工...]]></summary>
        <content type="html"><![CDATA[<p>所谓副业，本质上就是打多份工...</p><p>一定要和打工区别开来的话，可能就是我们如今所说的副业，更多的带上了对人生规划的一些思考。</p><h2>知道想做什么</h2><p>知道自己想做什么，永远是第一步。</p><p>我们常常听到别人高谈自己的&quot;规划&quot;，其实&quot;规划&quot;并不神秘。<br/>
<!-- -->你可以把他理解为一些简单的想法，或者说是对未来的一些想法，他们可以是想做的事，也可以是想去的地方...   </p><p>但事实是很多人不知道想做什么。<br/>
<!-- -->我们的第一份工作，往往和我们第一学历专业挂钩。<br/>
<!-- -->而大学毕业后，毕业生真正知道自己应该干什么的，其实寥寥无几。甚至可能工作两三年后才突然醒悟应该转投其他行业。</p><p>比如我本人是工程管理学专业，但我毕业后就投入到了互联网行业，因为我知道我不喜欢进工厂，相反我很喜欢计算机编程。<br/>
<!-- -->而我的大部分同学，毕业两三年后也陆陆续续的转行...</p><p>所以，我觉得很重要的一点，就是要想清楚自己现阶段到底需要做什么。<br/>
<!-- -->你需要去尝试做一个决定，这也许这不是一个长期的决定，但是它应该是现阶段你能够稳定投入精力的一个决定。</p><p>这个时候，也不需要考虑什么主业副业，能做好一件就已经很了不起了。</p><h2>为什么需要副业</h2><p>那为什么现在又有这么多人选择副业呢？</p><p>以我自己的情况来说，毫无疑问，是典型的属于需要依靠打工来维持生计的群体。个人睡后收入纯靠银行利息，工作的本质是为了混口饭吃。<br/>
<!-- -->当然我没有副业，因为互联网并不算特别轻松，我个人时间也并不是很充裕，很难花时间去打第二份工。
况且互联网收入相对来说是可以的。   </p><p>然而，这一切在我买房之后就改变了...<br/>
<!-- -->试想，当你的一半甚至一大半收入，全部用于还贷了，还得还三十年，一股有形的压力瞬间压得人喘不过气来。  </p><p>所以并不是我很闲，闲的要去找一份副业来充实自己，而是我很需要第二份收入来充实自己...<br/>
<!-- -->现在社会大环境又不好，工资过万，你可以打败全国一半以上的打工群体，但在一二线城市的房价物价面前，就是个屁。</p><p>所以，我想到的能坚持发展副业的人群：<br/>
<strong>1、</strong>入不敷出。<br/>
<strong>2、</strong>闲不下来。<br/>
<strong>3、</strong>个人爱好。    </p><p>大部分人应该是属于第一类吧... 但整体来看，我觉得时间和精力才是最终落地执行的决定因素。<br/>
<!-- -->因为过于透支自己的生命换来的财富，将是一笔无法偿还的高利贷。  </p><h2>认知自己的工作阶段</h2><p>当你有发展副业的想法时，我还是建议先梳理下你自己当前的工作情况，对于一份稳定的工作，应该大致朝着这个阶段方向发展：  </p><ul><li><strong>积累阶段</strong></li></ul><p>一般指一些刚入行的同学，在这个阶段他们需要去积累一些工作经验。</p><p>处于这个阶段时，会有一些比较明显的特点。<br/>
<!-- -->比如你的简历上面，可能需要编造大量技能和项目经验，以便通过 HR 的筛选。
一旦面试被人追着问细节时，就会暴露。</p><ul><li><strong>瓶颈阶段</strong></li></ul><p>一般指在某个领域有足够的经验。</p><p>处于这个阶段，你可能不在需要投递简历就能很轻松的找到一份工作。
如果工作不开心，你也很自信可以很快找到另一份工作，虽然找的不一定更好，但不用担心失业问题。</p><ul><li><strong>溢出阶段</strong></li></ul><p>一般指在某个领域有足够经验，在公司也处于中高层。</p><p>你可以很轻松的获取工作之外的报酬，比如公司分红等。</p><p>不是所有的工作都符合这三个阶段，但也可以大致比照着看看自己应该处于什么阶段，对于处于第一阶段的同学，我建议还是先投入精力处理好自己的工作，在考虑发展其他副业。</p><h2>关于副业的思考 - 给自己</h2><blockquote><p>不能盲目跟风！</p></blockquote><p>不管这句话适不适用于所有领域，但它一直都是我长期以来坚信的。虽然我很眼热能发展副业的同学们，但比起盲目发展副业，我觉得我应该先认清自己的情况。</p><p>首先有很多发展副业的人都是事业编制，他们相对来说个人时间比较充裕。<br/>
<!-- -->而互联网，是一个新兴技术频繁更替的行业，也是加班严重的行业之一，我时常也感觉自己个人时间不是很充裕。</p><p>其次，有部分牛人达到了工作的 <strong>瓶颈阶段</strong> 甚至是 <strong>溢出阶段</strong>，他们有能力、有资源去执行副业计划。<br/>
<!-- -->而我，菜鸟一个，短时间内是很难雄起了...</p><p>这样一看，我是完全不适合发展副业的了...
但是，生活环境又不会可怜我... 哎</p><p>基于这种背景，给自己一些建议：<br/>
<!-- -->1、多整理并总结自己的笔记。
2、整理文档时，优化下自己的文笔，不要乱糟糟了事，而应该尝试给别人讲述清楚。</p><p>说到底，短期还是只能提升软实力了</p><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍
:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Github 搭建 Docusaurus 站点]]></title>
        <id>Github 搭建 Docusaurus 站点</id>
        <link href="https://czasg.github.io/docusaurus/blog/2021/12/3/Github搭建Docusaurus站点"/>
        <updated>2021-12-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[本文章记录基于 Docusaurus 搭建静态站点，并通过 Github-Actions 实现自动部署。]]></summary>
        <content type="html"><![CDATA[<p>本文章记录基于 Docusaurus 搭建静态站点，并通过 Github-Actions 实现自动部署。</p><h2>1、新建 Github 仓库</h2><p>首先，登录<a href="https://github.com/new">Github仓库创建页面</a>，我们创建一个全新的空白仓库，项目名命名为 <code>testDocusaurus</code>。</p><p>然后通过 <code>git</code> 将仓库拉取到本地，至此，我们拥有了一个全新的空白仓库 <code>testDocusaurus</code>。  </p><p><img src="gitindex.png"/></p><h2>2、初始化 Docusaurus 项目</h2><blockquote><p><a href="https://docusaurus.io/zh-CN/docs/installation">Docusaurus项目初始化细节请参考官方文档</a></p></blockquote><p>进入到仓库 <code>testDocusaurus</code> 所在的空白目录，并在此目录打开终端界面。</p><p>为了更好的完成接下来的步骤，你可能需要预先安装<strong>nodejs</strong>。在此，假设你已经准备完毕。<br/>
<!-- -->那我们接下来通过以下指令初始化一个 <code>Docusaurus</code> 项目。</p><pre><code class="language-shell" metastring="script">&gt;&gt;&gt; npx create-docusaurus@latest website classic
...
...
Successfully created &quot;website&quot;.
Inside that directory, you can run several commands:

  npm start
    Starts the development server.

  npm run build
    Bundles your website into static files for production.

  npm run serve
    Serves the built website locally.

  npm deploy
    Publishes the website to GitHub pages.

We recommend that you begin by typing:

  cd website
  npm start

Happy building awesome websites!
</code></pre><p>该指令运行完后，会输出一些简单的运行指令，而且我们应该可以看到一个 <code>website</code> 的目录，我们先将里面的内容剪贴出来，放到我们空白仓库目录下面。</p><p>此时，我们按照提示，运行 <code>npm start</code> 指令，我们就可以运行此项目。</p><p>通常启动端口为3000，则默认路径为：http://localhost:3000/  </p><p><img src="websiteindex.png"/></p><h2>3、搭建 Github Actions</h2><blockquote><p>Github Actions部署细节参考<a href="https://docusaurus.io/zh-CN/docs/deployment#deploying-to-github-pages">官方文档</a></p></blockquote><p>接入 <code>Github Actions</code> 需要创建一对新的 <strong>SSH Key</strong>，并将公钥和密钥均配置到 Github，我们来具体看下操作。</p><p>首先创建密钥，我们可以指定一个新的目录，然后得到公钥（id_rsa.pub）和私钥（id_rsa）</p><pre><code class="language-shell" metastring="script">&gt;&gt;&gt; ssh-keygen -t rsa -C &quot;email&quot;
...
...
The key&#x27;s randomart image is:
+---[RSA 3072]----+
| .=oo*=o=o.      |
| o+.=o==.o       |
|  .O.O=+o        |
|  ..=o@.oo       |
| . o +.+S+..     |
|  . + . E.o      |
|   o . .         |
|  .              |
|                 |
+----[SHA256]-----+
</code></pre><p>我们打开仓库的 <code>deploy keys</code>，选择新增，将 <code>id_rsa.pub</code> 中的内容复制进去，并选中 <code>Allow write access</code> 框，表示赋予部署写权限。
<img src="deploykey.png"/></p><p>此时部署公钥已经完成，我们再将私钥也配置上。打开同级配置下的 Secret，选择新增密钥，
我们将私钥内容复制到 <code>Value</code> 中，而 <code>Name</code> 填写 <code>GH_PAGES_DEPLOY</code> 即可。</p><p>最后，我们创建 <code>Github Actions</code>，将模板复制进去，则整个流水线就已经配置好了。复制时，将对应的基础配置改下即可，如下：</p><pre><code class="language-shell" metastring="script">git config --global user.email &quot;email&quot;
git config --global user.name &quot;name&quot;
</code></pre><p>这里的 <code>email</code> 需要是 github 配置的 email，而 <code>name</code> 则是 github 用户名。</p><h2>4、更新仓库，尝试自动部署</h2><p>确保 Github Pages 已经初始化好，那么我们往仓库推送修改时，就可以触发自动部署了。</p><br/><p>:::info 👇👇👇
<strong>本文作者:</strong> Czasg<br/>
<strong>版权声明:</strong> 转载请注明出处哦~👮‍<br/>
<!-- -->:::</p>]]></content>
        <author>
            <name>Czasg</name>
            <uri>https://github.com/czasg</uri>
        </author>
    </entry>
</feed>